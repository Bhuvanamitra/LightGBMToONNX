{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e67bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from onnxruntime import InferenceSession\n",
    "import onnxruntime as rt\n",
    "from sklearn.base import ClassifierMixin\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, Dataset, train, Booster\n",
    "from skl2onnx import update_registered_converter\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx.common.shape_calculator import (\n",
    "    calculate_linear_classifier_output_shapes,  # noqa\n",
    "    calculate_linear_regressor_output_shapes,\n",
    ")\n",
    "from onnxmltools.convert.lightgbm.operator_converters.LightGbm import (\n",
    "    convert_lightgbm  # noqa\n",
    ")\n",
    "import onnxmltools\n",
    "from onnxmltools.convert.lightgbm._parse import WrappedBooster  # noqa\n",
    "from skl2onnx import to_onnx\n",
    "from skl2onnx._parse import (\n",
    "    _parse_sklearn_classifier, _parse_sklearn_simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33799e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lightgbm_output_shapes(operator):\n",
    "    op = operator.raw_operator\n",
    "    if hasattr(op, \"_model_dict\"):\n",
    "        objective = op._model_dict['objective']\n",
    "    elif hasattr(op, 'objective_'):\n",
    "        objective = op.objective_\n",
    "    else:\n",
    "        raise RuntimeError(  # pragma: no cover\n",
    "            \"Unable to find attributes '_model_dict' or 'objective_' in \"\n",
    "            \"instance of type %r (list of attributes=%r).\" % (\n",
    "                type(op), dir(op)))\n",
    "    if objective.startswith('binary') or objective.startswith('multiclass'):\n",
    "        return calculate_linear_classifier_output_shapes(operator)\n",
    "    if objective.startswith('regression'):  # pragma: no cover\n",
    "        return calculate_linear_regressor_output_shapes(operator)\n",
    "    raise NotImplementedError(  # pragma: no cover\n",
    "        \"Objective '{}' is not implemented yet.\".format(objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb95735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_parser(scope, model, inputs, custom_parsers=None):\n",
    "    if hasattr(model, \"fit\"):\n",
    "        raise TypeError(  # pragma: no cover\n",
    "            \"This converter does not apply on type '{}'.\"\n",
    "            \"\".format(type(model)))\n",
    "\n",
    "    if len(inputs) == 1:\n",
    "        wrapped = WrappedBooster(model)\n",
    "        objective = wrapped.get_objective()\n",
    "        if objective.startswith('binary'):\n",
    "            wrapped = WrappedLightGbmBoosterClassifier(wrapped)\n",
    "            return _parse_sklearn_classifier(\n",
    "                scope, wrapped, inputs, custom_parsers=custom_parsers)\n",
    "        if objective.startswith('multiclass'):\n",
    "            wrapped = WrappedLightGbmBoosterClassifier(wrapped)\n",
    "            return _parse_sklearn_classifier(\n",
    "                scope, wrapped, inputs, custom_parsers=custom_parsers)\n",
    "        if objective.startswith('regression'):  # pragma: no cover\n",
    "            return _parse_sklearn_simple_model(\n",
    "                scope, wrapped, inputs, custom_parsers=custom_parsers)\n",
    "        raise NotImplementedError(  # pragma: no cover\n",
    "            \"Objective '{}' is not implemented yet.\".format(objective))\n",
    "\n",
    "    # Multiple columns\n",
    "    this_operator = scope.declare_local_operator('LightGBMConcat')\n",
    "    this_operator.raw_operator = model\n",
    "    this_operator.inputs = inputs\n",
    "    var = scope.declare_local_variable(\n",
    "        'Xlgbm', inputs[0].type.__class__([None, None]))\n",
    "    this_operator.outputs.append(var)\n",
    "    return lightgbm_parser(scope, model, this_operator.outputs,\n",
    "                           custom_parsers=custom_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e21044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedLightGbmBoosterClassifier(ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Trick to wrap a LGBMClassifier into a class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wrapped):  # pylint: disable=W0231\n",
    "        for k in {'boosting_type', '_model_dict', '_model_dict_info',\n",
    "                  'operator_name', 'classes_', 'booster_', 'n_features_',\n",
    "                  'objective_', 'boosting_type', 'n_features_'}:\n",
    "            if hasattr(wrapped, k):\n",
    "                setattr(self, k, getattr(wrapped, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a22c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgm_model = Booster(model_file='model.txt')\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(lgm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4816c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', \"rb\") as mp:\n",
    "    lgbm_model: LGBMClassifier = pickle.load(file=mp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a84a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model.num_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92da88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_registered_converter(\n",
    "            WrappedLightGbmBoosterClassifier,\n",
    "            'WrappedLightGbmBoosterClassifier',\n",
    "            calculate_lightgbm_output_shapes,\n",
    "            convert_lightgbm, parser=lightgbm_parser,\n",
    "            options={'zipmap': [False, True], 'nocl': [False, True]})\n",
    "update_registered_converter(\n",
    "            WrappedBooster, 'WrappedBooster',\n",
    "            calculate_lightgbm_output_shapes,\n",
    "            convert_lightgbm, parser=lightgbm_parser,\n",
    "            options={'zipmap': [False, True], 'nocl': [False, True]})\n",
    "update_registered_converter(\n",
    "            Booster, 'LightGbmBooster', calculate_lightgbm_output_shapes,\n",
    "            convert_lightgbm, parser=lightgbm_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3838bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_registered_converter(\n",
    "            WrappedLightGbmBoosterClassifier,\n",
    "            'WrappedLightGbmBoosterClassifier',\n",
    "            calculate_lightgbm_output_shapes,\n",
    "            convert_lightgbm, parser=lightgbm_parser,\n",
    "            options={'zipmap': [False, True], 'nocl': [False, True]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "503c6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = to_onnx(lgbm_model, initial_types=[('feature_input', FloatTensorType([None, 2021]))],\n",
    "            options={WrappedLightGbmBoosterClassifier: {'zipmap': False}}, target_opset={'': 15, 'ai.onnx.ml': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf8bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
